<|im_start|>system
You are a helpful instruction-following assistant and an expert in evaluation of LLM outputs using rubrics.
<|im_end|>
<|im_start|>user
We are conducting a holistic evaluation of powerful large language models (LLMs). Your task is to create a detailed analytic rubric for grading LLM outputs on a given assignment. The rubric must be comprehensive, objective, and clear, designed to be used by crowd-workers who lack domain-specific knowledge. The rubric should be very detailed/specific to the assignment and contain examples of answers (e.g. dates, events, names, correct math solutions, etc.) that would would fall into each category, so that crowd-workers with little knowledge of the domain (or even LLM-judges) can easily apply it in a consistent manner. 

When creating the rubric, adhere to best practices for analytic rubrics, such as:
- use specific and measurable criteria
- provide detailed performance level descriptions: avoid vague terms like "good" or "adequate." Instead, refer to specific behaviors or outputs expected at each level so that crowd-workers can easily apply it in a consistent manner.
- ensure objectivity: descriptions should be unambiguous, objective and designed in a way that different evaluators can apply it consistently.
- reference concrete elements: when possible, mention specific dates, events, names, or other information the LLM should include in its response so that non-experts can apply the rubric without domain-specific knowledge.
- use distinct performance levels: clearly differentiate between "Excellent," "Good," "Fair," and "Poor" performance for each criterion.

Based on the provided "assignment" and "brainstormed rubric" (things to consider when writing the rubric) and "brainstormed response" (if available, an example of a strong LLM response) generate a JSON with output containing the following:
- rubric (list[dict[str, str]]): A list of dictionaries, each corresponding to a criterion in the rubric. Each dictionary should have the following keys:
  - criterion (str): The name of the criterion.
  - weight (int): The weight of the criterion in the total score, expressed as an integer between 0-100.
  - performance_to_description (dict[str, str]): A mapping between the performance level ("excellent", "good", "fair", "poor") and a detailed description of what the LLM needs to do to achieve that rating for that criterion. Descriptions must be comprehensive, clear, and refer to specific parts of the ideal response for the criterion. They should be written in a way that crowd-workers can easily understand and apply to the LLM's response.
- excellent_response (str): An example of a response that achieves the "excellent" rating for each criterion.

The json output should then be given to `evaluate_llm(json_output)` to evaluate the llm.

## Assignment
{instruction}

## Braistormed Rubric
{brainstormed_rubric}

## Brainstormed Response (optional)
{brainstormed_response}

Now generate the rubric using `evaluate_llm(json_output)`.

<|im_end|>