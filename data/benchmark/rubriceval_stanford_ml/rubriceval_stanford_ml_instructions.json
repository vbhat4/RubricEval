[
  {
    "category":         "Stats & ML",
    "short_name":       "tokenization",
    "instruction":      "Before passing text through a transformer, the text\nis often processed into tokens corresponding to common subwords. What is\none clear/unarguable benefit of tokenizing subwords compared to tokenizing\neach character (or byte) separately?",
    "expert_solution": "Subword tokenization results in shorter sequences, so faster training/inference, especially given the O(n^2) cost of transformers. Any answer that seems plausible but talks about expressiveness and data efficiency should only get half a point as it's not the main benefit."
  },
  {
    "category":         "Stats & ML",
    "short_name":   "transformer",
    "instruction":      "Think of a transformer called MyT that has the following parameters \n\nIt had:\n\\begin{itemize}\n    \\item $57$ transformer blocks, i.e. $N$ in the figure.\n    \\item model dimensionality of $d=1756$, i.e. all the inputs, outputs and activations of transformer blocks are vectors in $\\mathbb{R}^{d}$. In particular, the Key, Value, Query, and Output matrices are in $\\mathbb{R}^{d\\times d}$.\n    \\item a max context length of $C=2800$, i.e. there are $C$ positional embeddings in $\\mathbb{R}^{d}$.\n    \\item 439 attention heads.\n    \\item a vocabulary size of $V=63257$, i.e., there are $V$ input embeddings in $\\mathbb{R}^{d}$, and the last linear layer is a softmax over $V$ classes.\n    \\item the feedforward model is two layer MLP with temporary hidden activations of size $\\mathbb{R}^{4*d}$, i.e. the computational graph is $input\\ (\\mathbb{R}^{d}) \\to \\mathrm{\\mathbf{linear}} \\to tmp\\ (\\mathbb{R}^{4*d}) \\to \\mathrm{\\mathbf{ReLU}} \\to tmp\\ (\\mathbb{R}^{4*d}) \\to \\mathrm{\\mathbf{linear}}  \\to output\\ (\\mathbb{R}^{d})$ where layers are in bold and there's no bias in Linear layers.\n\\end{itemize}\n\nWhere do most of the parameters come from in a MyT?\nYour answer should be one of the following:\nThe input embeddings, the positional embeddings, all the multi-head attention blocks, all the normalization layers, all the feedforward blocks, the final linear layer. How many parameters does that part have?",
    "expert_solution": "Feedforward blocks require $2*4*LD^2$ parameters which is the bottleneck (2 layers per block, each with $4D^2$ parameters). In contrast, all the self-attention layers require $4LD^2$ parameters (one for K,V,Q,O). All other layers have a negligible amount of parameters. Putting all the numbers in, we have $2*4*1756^2*57 = 1406092416$ parameters for the feedforward blocks."
  },
  {
    "category":         "Stats & ML",
    "short_name":   "least_squares",
    "instruction":      "\nThe Least Squares objective is denoted by:\n\n\\begin{align*}\n    \\argmin_{w} \\quad ( Y - Xw)^T(Y - Xw)\n\\end{align*}\n\nwhere $Y \\in \\mathbb{R}^n$, $w \\in \\mathbb{R}^d$ are the parameters, and $X \\in \\mathbb{R}^{n \\times d}$ is the feature matrix.\\\\\n\nThe Ridge-regression regularized objective is:\n\\begin{align*}\n\\argmin_{w} \\quad (Y - Xw)^T(Y - Xw) + \\lambda w^Tw\n\\end{align*}\n\nShow that $||w^*_r ||_2^2 \\leq ||w^*_l ||_2^2$. Where $w^*_l$ and $w^*_r$ respectively denote the optimal solution for the least squares objective and the ridge regression objective.\\\\",
    "expert_solution": "Let $x$ be a fixed design matrix and $y$ a fixed response vector. Let $V \\Sigma U^{T}$ be the SVD of $x$ and $U \\Lambda U^{T}$ be the eigendecomposition of $x^{T} x$, where it is important to recall that $\\Lambda$ has no negative entries, i.e. $x^{T} x$ is positive semidefinite, and also $\\Sigma^2 = \\Lambda$. Recall that $U$ and $V$ are orthogonal matrices, and that $\\Lambda$ and $\\Sigma$ are diagonal.\n\nThe ridge estimator with penalty parameter $\\lambda$ has coefficients\n$$\\hat{\\beta}_{\\text{ridge}}^{\\lambda} = (x^{T} x + \\lambda I)^{-1} x^{T} y.$$\nWe can compute its squared norm by plugging in the factorization above. \n$$\\|\\hat{\\beta}_{\\text{ridge}}^{\\lambda}\\|^{2} = (\\hat{\\beta}_{\\text{ridge}}^{\\lambda})^{T}\\hat{\\beta}_{\\text{ridge}}^{\\lambda} = y^{T} x (x^{T} x + \\lambda I)^{-2} x^{T} y$$\n$$ = y^{T} V \\Sigma U^{T} (U \\Lambda U^{T} + \\lambda I)^{-2} U \\Sigma V^{T} y$$\n$$ = y^{T} V \\Sigma U^{T} (U (\\Lambda + \\lambda I) U^{T} )^{-2} U \\Sigma V^{T} y$$\n$$ = y^{T} V \\Sigma U^{T} (U (\\Lambda + \\lambda I)^{-2} U^{T} ) U \\Sigma V^{T} y$$\n$$ = y^{T} V \\frac{\\Sigma^{2}}{(\\Lambda + \\lambda I)^{2}} V^{T} y.$$\nI'm abusing notation a bit in this last inequality by writing these matrices as a fraction, but I really just mean pointwise division of their elements since all of the matrices involved are diagonal. Next, substituting in that the eigenvalues of $x^{T} x$ are the squared singular values of $x$, we have\n$$= y^{T} V \\frac{ \\Lambda}{(\\Lambda + \\lambda I)^{2}} V^{T} y.$$\nDenote $V^{T} y$ by $w$. We have\n$$y^{T} V \\frac{ \\Lambda}{(\\Lambda + \\lambda I)^{2}} V^{T} y = w^{T} \\frac{ \\Lambda}{(\\Lambda + \\lambda I)^{2}} w$$\n$$ = \\sum_{i=1}^{p} \\frac{\\Lambda_{i,i}}{(\\Lambda_{i,i} + \\lambda)^{2}} w_{i}^{2}.$$\n\nTherefore, for two penalty parameters, $\\lambda_{1} \\leq \\lambda_{2}$, we have\n$$\\|\\hat{\\beta}_{\\text{ridge}}^{\\lambda_{2}}\\|^{2} \\leq \\|\\hat{\\beta}_{\\text{ridge}}^{\\lambda_{1}}\\|^{2}  \\iff \\sum_{i=1}^{p} \\frac{\\Lambda_{i,i}}{(\\Lambda_{i,i} + \\lambda_{2})^{2}} w_{i}^{2} \\leq \\sum_{i=1}^{p} \\frac{\\Lambda_{i,i}}{(\\Lambda_{i,i} + \\lambda_{1})^{2}} w_{i}^{2}.$$\nThis last inequality is true because $\\frac{\\Lambda_{i,i}}{(\\Lambda_{i,i} + \\lambda_{2})^{2}} \\leq \\frac{\\Lambda_{i,i}}{(\\Lambda_{i,i} + \\lambda_{1})^{2}}$ for each $i \\in \\{1,...,p\\}$, which follows because $\\Lambda_{i,i} \\geq 0$."
  },
  {
    "category":         "Stats & ML",
    "short_name":   "quantile",
    "instruction":      "You must now predict the value of a random variable, $Y$, which is \\textbf{continuous} valued so as to minimize the expected loss over the distribution of $Y$, i.e. $\\mathbb{E}_{Y}[L(\\hat{Y}, Y)]$. You know the entire probability density function of $Y$, i.e. $p(Y)$ is fully known to you. You can also assume that $Y$ has a Lebesgue density.  \n\nIn the case where you are using the following loss function as your objective: \n\n```latex\n\\begin{align*}\n    L(\\hat{Y}, Y) &= \\left| \\left( Y - \\hat{Y} \\right) \\left(0.16- \\mathbf{1}\\{ Y > \\hat{Y} \\} \\right)\n\\end{align*}\n``` \n\nwhat is the optimal prediction for $\\hat{Y}$? Prove it.",
    "expert_solution": "To solve for the minimum of the loss function\n\n```latex\n\\[\n\\min_{\\hat{Y}} E_{p(Y)} \\left[ \\left| \\left( Y - \\hat{Y} \\right) \\left(0.16 - \\mathbf{1}\\{ Y > \\hat{Y} \\} \\right) \\right| \\right],\n\\]\n```\n\nwe need to determine the optimal estimator \\(\\hat{Y}\\) that minimizes the expected loss.\n\nAfter a bit of simplification, we can rewrite the task as:\n\n```latex\n\\[\n\\arg\\min_{\\hat{Y}} E_{p(Y)}[L(\\hat{Y}, Y)] = \\int_{-\\infty}^{\\hat{Y}} 0.16 (\\hat{Y} - Y) p(Y) \\, dY + \\int_{\\hat{Y}}^{\\infty} 0.84 (Y - \\hat{Y}) p(Y) \\, dY.\n\\]\n```\n\nTo find the optimal \\(\\hat{Y}\\), take the derivative with respect to \\(\\hat{Y}\\) and set it to zero:\n\n```latex\n\\begin{align}\n 0&=\\frac{d}{d\\hat{Y}} E_{p(Y)}[L(\\hat{Y}, Y)]\\\\\n &=  \\frac{d}{d\\hat{Y}} \\left( \\int_{-\\infty}^{\\hat{Y}} 0.16 (\\hat{Y} - Y) p(Y) \\, dY + \\int_{\\hat{Y}}^{\\infty} 0.84 (Y - \\hat{Y}) p(Y) \\, dY \\right) \\\\\n    &= 0.16 \\int_{-\\infty}^{\\hat{Y}} p(Y) \\, dY - 0.84 \\int_{\\hat{Y}}^{\\infty} p(Y) \\, dY & \\text{by Leibniz's rule} \\\\\n     &= 0.16 P(Y \\leq \\hat{Y}) - 0.84 P(Y > \\hat{Y}) \\\\\n &=  0.16 F(\\hat{Y}) - 0.84 (1 - F(\\hat{Y})) & P(Y \\leq \\hat{Y}) + P(Y > \\hat{Y}) \\\\\n 0.84 &= 0.16 F(\\hat{Y}) + 0.84 F(\\hat{Y})  \\\\\n    \\hat{Y}) &= F^{-1}(0.84).\n```\n\n\n   where \\(F(\\hat{Y}) = P(Y \\leq \\hat{Y})\\) is the cumulative distribution function (CDF) of \\(Y\\) so \\(F^{-1}(0.84)\\) is the 84th percentile of the distribution of \\(Y\\).\n"
  },
  {
    "category":         "Stats & ML",
    "short_name":   "rl",
    "instruction":      "Consider a simple grid world environment. The environment is a 3x3 grid, where each cell represents a state \\( s \\in \\{s_1, s_2, \\ldots, s_9\\} \\), going top-to-bottom and left-to-right.\nThe top-left corner of the grid is $s_1$, and the bottom-right is $s_9$.\nThe top-middle is $s_2$, the top-right is $s_3$, etc.\n\nThe agent can perform four possible actions in each state: up, down, left, and right. Actions that would move the agent off the grid have no effect (the agent stays in the same state).\n\nThe agent starts in the top-left corner of the grid (\\( s_1 \\)) and aims to reach the bottom-right corner (\\( s_9 \\)). The reward function \\( R(s, a) \\) is defined as follows:\n\n\\begin{itemize}\n    \\item \\( R(s, a) = 1 \\) if \\( s = s_9 \\)\n    \\item \\( R(s, a) = -0.1 \\) for all other states \\( s \\neq s_9 \\)\n\\end{itemize}\n\nAssume the agent follows an optimal policy and the discount factor $\\gamma = 0.9$\n\nDetermine the optimal action \\( a^*(s) \\) for the agent in state \\( s_5 \\) (the center of the grid) after performing one step of value iteration (assume we initialize $V(s_i) = 0$ for all states). What is the expected reward of taking this action?",
    "expert_solution": "The environment is deterministic, so our value iteration update rule is:\n$$ V'(s) = \\max_a  R(s, a) + \\gamma V(s') $$\n\nWhich gives us:\n\\begin{align*}\n    V'(s_2) &= \\max(-0.1 + 0.9 \\cdot 0, -0.1 + 0.9 \\cdot 0, -0.1 + 0.9 \\cdot 0) = -0.1\\\\\n    V'(s_4) &= \\max(-0.1 + 0.9 \\cdot 0, -0.1 + 0.9 \\cdot 0, -0.1 + 0.9 \\cdot 0) = -0.1\\\\\n    V'(s_6) &= \\max(-0.1 + 0.9 \\cdot 0, -0.1 + 0.9 \\cdot 0) = -0.1\\\\\n    V'(s_8) &= \\max(-0.1 + 0.9 \\cdot 0, -0.1 + 0.9 \\cdot 0, 1 + 0.9 \\cdot 0) = 1\n\\end{align*}\n\nPossible actions from \\( s_5 \\):\n\\begin{itemize}\n    \\item Up (to \\( s_2 \\)): \\( R(s_5, \\text{up}) + \\gamma V'(s_2) = -0.1 + 0.9 \\cdot -0.1 = -0.19 \\)\n    \\item Left (to \\( s_4 \\)): \\( R(s_5, \\text{left}) + \\gamma V'(s_4) = -0.1 + 0.9 \\cdot -0.1 = -0.19 \\)\n    \\item Right (to \\( s_6 \\)): \\( R(s_5, \\text{right}) + \\gamma V'(s_6) = -0.1 + 0.9 \\cdot -0.1 = -0.19 \\)\n    \\item Down (to \\( s_8 \\)): \\( R(s_5, \\text{down}) + \\gamma V'(s_8) = -0.1 + 0.9 \\cdot 1 = 0.8 \\)\n\\end{itemize}\n\nThe optimal action \\( a^*(s_5) \\) is \"down\" with the value of 0.8."
  },
  {
    "category":         "Stats & ML",
    "short_name":   "backprop",
    "instruction":      "You have just joined a robotics startup focused on developing autonomous delivery robots designed to operate in complex urban environments. One of your first tasks is to build a machine learning model that can classify and predict the types of obstacles the robot might encounter on the sidewalk. The robot is equipped with multiple sensors that collect various data inputs such as distance measurements, speed, surface texture, and environmental noise levels.\n\nGiven $d$ inputs (e.g., sensor readings for distance, speed, texture, etc.), your model must identify, at regular intervals, which of the $c$ possible obstacles (e.g., pedestrian, bicycle, trash can, etc.) are present in the robot's path. The classification model will be a multi-label binary classifier since multiple obstacles could appear simultaneously.\n\nHowever, the startup's custom-built robots use a proprietary software stack that isn't compatible with standard machine learning libraries like PyTorch or TensorFlow. As a result, you must implement both the forward and backward pass in numpy to ensure the model can be deployed directly onto the robot's onboard system.\n\nYou decide to implement a simple 3-layer feedforward neural network with the following architecture:\n\n```latex\n$input\\ (\\mathbb{R}^{d}) \n\\to \\mathrm{\\mathbf{linear}} \n\\to \\mathrm{\\mathbf{ScaledReLU}} \n\\to tmp\\ (\\mathbb{R}^{h}) \n\\to \\mathrm{\\mathbf{linear}} \n\\to \\mathrm{\\mathbf{ScaledReLU}} \n\\to tmp\\ (\\mathbb{R}^{h}) \n\\to \\mathrm{\\mathbf{linear}}  \n\\to logits\\ (\\mathbb{R}^{c})\n\\to \\mathrm{\\mathbf{Sigmoid}}\n\\to prediction \\ (\\mathbb{R}^{c})\n$\n```\n\nThe loss function to be used for training this network is the mean negative log-likelihood. Below is the final structure of the MLP class. Your job is to fill in the definitions of the functions called by the MLP class, ensuring they are compatible with the robot's custom software environment.\n\n```python\nclass MLP:\n    def __init__(self, d, h, c):\n        self.layers = [\n            Linear(d, h), ScaledReLU(h),  # input_layer\n            Linear(h, h), ScaledReLU(h),  # hidden_layer\n            Linear(h, c), Sigmoid(c)  # output_layer\n        ]\n        \n    def forward(self, X):\n        \"\"\"Predict classes for a batch of n examples\"\"\"\n        out = X  # $\\in \\mathbb{R}^{n \\times d}$\n        for layer in self.layers:\n            out = layer.forward(out) \n        return out\n    \n    def sgd(self, X_train, Y_train):\n        # Train each layer of the MLP with SGD\n        n_train = len(X_train)\n        epochs = 10\n        bs = 100  # batch_size\n        for _ in range(epochs):\n            for b_idx in range(0, n_train, bs):\n                X_batch = X_train[b_idx:b_idx + bs]\n                Y_batch = Y_train[b_idx:b_idx + bs]\n                # update all parameters\n                self.single_step_sgd(X_batch, Y_batch)\n\n    def single_step_sgd(self, X, Y):  # $X\\in \\mathbb{R}^{n\\times d}$, $Y\\in \\mathbb{R}^{n\\times c}$\n        \"\"\"Train the MLP with a single step of batched SGD.\"\"\"\n        Y_hat = self.forward(X)  # $\\in \\mathbb{R}^{n \\times c}$\n        loss_nabla_O = get_grad_log_loss(Y, Y_hat)\n        for layer in self.layers[::-1]:  # backward pass reverses layers\n            loss_nabla_O, *param_grads = layer.backward(loss_nabla_O)\n            layer.update_params_(*param_grads, lr=0.1)\n```\n\nIn the following sections, you should write the `forward`, `backward`, `update_params_`, and `__init__` methods for the `Linear`, `ScaledReLU`, and `Sigmoid` classes, as well as the `get_grad_log_loss` function. Note that `ScaledReLU` is similar to ReLU, but it scales the output by 2 if the input is greater than 1 and 0 otherwise. For the linear layer, use Kaiming initialization for the weights and zero initialization for the bias. Use activation checkpointing to save memory wherever possible. Implement everything using NumPy. Your code should be fully functional.\n\nConsidering only matrix multiplications in your code, approximately how much slower (in terms of FLOPs) would it be to train our MLP on $n = 10,000$ examples versus performing inference on $n = 10,000$ examples? Your answer should be an integer (round it if you get a float). Provide your derivation.",
    "expert_solution": "```python\nimport numpy as np\nclass Sigmoid:\n    def __init__(self, c):\n        self.c = c\n        \n    def forward(self, X):\n        O = 1 / (1 + np.exp(-X))\n        self.O = O\n        return O\n\n    def backward(self, loss_nabla_O):      \n        loss_nabla_X = loss_nabla_O * self.O * (1-self.O)\n        return loss_nabla_X\n    \n    def update_params_(self, lr):\n        pass # Sigmoid has no parameters\n\nclass ScaledReLU:\n    def __init__(self, h):\n        self.h = h\n        \n    def forward(self, X):\n        self.mask = (X > 1).astype(X.dtype) * 2\n        return self.mask * X \n\n    def backward(self, loss_nabla_O):\n        loss_nabla_X = loss_nabla_O * self.mask\n        return loss_nabla_X\n\n    def update_params_(self, lr):\n        pass # ReLU has no parameters\n\nclass Linear:\n    def __init__(self, d, h):\n        self.W = np.random.randn(d, h) * np.sqrt(2/d)\n        self.b = np.zeros(h)\n        \n    def forward(self, X):\n        self.X = X\n        return X @ self.W + self.b\n\n    def backward(self, loss_nabla_O):\n        loss_nabla_W = self.X.T @ loss_nabla_O\n        loss_nabla_b = loss_nabla_O.sum(axis=0)\n        loss_nabla_X = loss_nabla_O @ self.W.T\n        return loss_nabla_X, loss_nabla_W, loss_nabla_b\n\n    def update_params_(self, loss_nabla_W, loss_nabla_b, lr):\n        self.W -= lr * loss_nabla_W\n        self.b -= lr * loss_nabla_b\n\ndef get_grad_log_loss(Y, Y_hat, eps=1e-8):\n    return 1/len(Y) * (Y_hat-Y)/((Y_hat * (1-Y_hat))+ eps)\n```\n\nComplexity: only linear layer has matrix multiplications so we should only consider that. As a reminder, the FLOPs for multiplication of two matrices of shape $n \\times o$ and $i \\times o$ is approximately $2nio$ because there are $no$ dot products, each for vectors of shape $i$ and dot product two vectors of dimensionality $d$ is approx $2d$ (one $d$ for mult, one $d-$ for add). The forward for the linear thus requires $2nio$ FLOPs. The backward requires $4nio$ FLOPs: $2nio$ for the gradients w.r.t. inputs $2nio$ for gradients w.r.t. weights. So $4nio$. Training requires both the backward and the forward pass so $6nio$ in total. Inference only requires the forward pass so $2nio$. The ratio is $\\frac{6nio}{2nio}=3$ so training requires 3x more compute than inference. This is independent of $n$ (or rather large $n$ is equivalent to only considering matrix multiplications)."
  },
  {
    "category":         "Stats & ML",
    "short_name":   "variance",
    "instruction":      "You are a data analyst working for a tech company that manages a fleet of autonomous delivery drones. These drones deliver packages in two distinct environments: a densely packed urban area and a sprawling suburban area. Your task is to predict the delivery times of these drones to optimize scheduling and improve customer satisfaction.\n\n**Urban Area**: In the urban environment, the drone delivery times are primarily influenced by a range of factors such as tall buildings, varying wind speeds due to narrow alleyways, and frequent stops. This results in a highly variable delivery time that is longer in some areas and shorter in others, but generally follows a pattern that could be represented by a Trapezoidal Distribution. The delivery times are likely to increase linearly up to a certain point due to navigating traffic and wind, remain relatively stable when in open spaces, and then decrease linearly as they approach more predictable pathways near delivery points. We assume that for the Trapezoidal distribution a=0, b=1, c=3, and d=4. As a reminder the Trapezoidal Distribution is defined as follows:\n\n```latex\n\\begin{align}\nf_{urban}(x)=\n\\begin{cases}\n\\frac{2}{d+c-a-b}\\frac{x-a}{b-a}  & \\text{for } a\\le x < b \\\\\n\\frac{2}{d+c-a-b}  & \\text{for } b\\le x < c \\\\\n\\frac{2}{d+c-a-b}\\frac{d-x}{d-c}  & \\text{for } c\\le x \\le d\n\\end{cases}\n\\end{align}\n```\n\n**Suburban Area**: In the suburban environment, drone delivery times are less variable due to more consistent and open flying conditions. The drones generally fly at a constant speed over open areas with fewer obstacles. This scenario is well-modeled by a Normal Distribution with a specific mean and variance, where most delivery times cluster around the mean due to the predictable nature of flying over suburban landscapes. We assume that the mean delivery time is 5 hours with a standard deviation of 0.1 hours. Namely:\n\n```latex\n\\begin{align}\nf_{suburban}(x) = \\frac{1}{\\sqrt{2\\pi} \\cdot 0.5} \\cdot e^{-\\frac{(x - 2)^2}{2 \\cdot 0.5^2}}\n\\end{align}\n```\n\nAssume that 70% of the drones operate in the urban area and 30% in the suburban area. So the delivery times are a mixture of the two distributions. What is the mean and variance of the delivery times?\n\n",
    "expert_solution": "To calculate the variance of the delivery times, we can use the following formula for the variance of a mixture of distributions:\n\n```latex\n\\begin{align}\n\\operatorname{Var}[X] & = \\sigma^2 \\\\\n& = \\operatorname{E}[X^2] - \\mu^{2} \\\\\n& = \\sum_{i=1}^n w_i(\\sigma_i^2 + \\mu_i^{2} )- \\sum_{i=1}^n w_i\\mu_i^{2}\n\\end{align}\n```\n\nso we just need to calculate the variance and mean of each distribution.\n\nFor the Gaussian distribution we have $\\mu_g=5$ and variance $\\sigma_g^2=0.01$.\n\nFor the Trapezoidal distribution you can compute $E[X]$ and $E[X^2]$ by integrating the distribution function on the entire range (i.e. 3 simple intergrals). Or you can use\nthe following formula for the $k$-th moment of the Trapezoidal distribution:\n\n```latex\n\\begin{align}\nE[X^k] = \\frac{2}{d+c-b-a}\\frac{1}{(k+1)(k+2)}\\left(\\frac{d^{k+2} - c^{k+2}}{d - c} - \\frac{b^{k+2} - a^{k+2}}{b - a}\\right)\n\\end{align}\n```\n\nUsing this formula with k=1, a=0, b=1, c=3, and d=4 \n\n```latex\n\\begin{align}\n\\mu_t &= E[X] \\\\  \n&= \n\\frac{1}{3(d+c-b-a)}\\left(\\frac{d^3 - c^3}{d - c} - \\frac{b^3 - a^3}{b - a}\\right)\\\\ \n& = 2\n\\end{align}\n```\n\nand \n\n```latex\n\\begin{align}\n\\sigma_t^2\n&= E[X^2] - \\mu_t^2\\\\\n&= \\frac{1}{6(d+c-b-a)}\\left(\\frac{d^4 - c^4}{d - c} - \\frac{b^4 - a^4}{b - a}\\right) - 4\\\\\n&= \\frac{29}{6}- 4^2\\\\\n&= \\frac{5}{6}\n\\end{align}\n```\n\nPutting all together we have that the mean of the mixture is \n\n```latex    \n\\begin{align}\n\\mu  &= \\sum_{i=1}^n w_i\\mu_i\\\\\n& = 0.3 * 5 + 0.7 * 2\\\\\n& = 2.9\n\\end{align}\n``` \n\nand the variance of the mixture is \n\n```latex\n\\begin{align}\n\\sigma^2  &= \\sum_{i=1}^n w_i(\\sigma_i^2 + \\mu_i^{2} )- \\mu^2 \\\\\n& = 0.3 * (0.01 + 5^2) + 0.7 * 29/6 - 2.9^2\\\\\n& = 2.4763\n\\end{align}\n```\n"
  },
  {
    "category":         "Stats & ML",
    "short_name":   "optimization",
    "instruction":      "\nIn online convex optimization the setting is the following:\n- the learner makes a decision $x_t \\in \\mathcal{D} \\subseteq \\mathbb{R}^d$\n- nature gives a convex function $f_t : \\mathcal{D} \\to \\mathbb{R}$\n- the learner incurs a loss $f_t(x_t)$ based on the selected decision\n- the learner uses this feedback to update its decision-making strategy and we start a new round\n\nThe goal of the learner is to minimize the cumulative loss over the entire sequence of rounds, compared to the best \\textit{fixed} decision in hindsight. \nThe regret of a learning algorithm mapping previous $\\mathcal{A}: \\{f_1, \\dots, f_{t_1}\\} \\to x_t^{\\mathcal{A}}$ losses to a current prediction is the worst loss over possible function from nature.\n\n```latex\n\\begin{equation}\n\\mathrm{Reg}_T(\\mathcal{A}) = \\sup_{\\{f_1, \\dots, f_T\\}} [\\sum_t^T f_t(x_t^{\\mathcal{A}}) - \\min_{x \\in \\mathcal{X}} \\sum_t^T f_t(x)]\n\\end{equation}\n```\n\nNote that the regret is always positive. \nIdeally, the average per-round regret should go down to zero as the number of rounds increases, meaning that you tend to the best fixed strategy.\nSlighlty more formally, we would like the regret to be sublinear as a function of T, i.e, $\\mathrm{Reg}_T(\\mathcal{A}) = o(T)$.\n\nProve that under reasonable assumptions the online projected subgradient descent algorithm achieves sublinear regret. State your assumptions and prove the regret bound.",
    "expert_solution": "A very natural and simple algorithm for online convex optimization is to run online gradient descent.\nThe two challenges main challenges are that (1) we have to ensure that the decisions are in the feasible set $\\mathcal{D}$; and (2) the gradients of the function $f_t$ may not be defined. \nTo alleviate these issues we thus use online projected subgradient descent. \nAs a reminder, a subgradient $\\nabla f_t(x_t)$ of a convex function $f_t$ at $x_t$ satisfies, for all $x' \\in \\mathcal{D}$\n\n```latex\n\\begin{equation}\nf_t(x') - f_t(x_t) \\geq \\nabla f_t(x_t) \\cdot (c' - c_t).\n\\end{equation}\n```\n\nA subgradient always exists for a convex function but it may not be unique. \nGiven a subgradient, we can then perform projected subgradient descent to ensure that $x_{t+1} \\in \\mathcal{D}$.\n\n```latex\n\\begin{equation}\nx_{t+1} = \\Pi_{\\mathcal{D}} [x_t - \\eta \\nabla f_t(x_t)]\n\\end{equation}\n```\n\nwhere $\\eta$ is the learning rate and $\\Pi_{\\mathcal{D}}$ projects the result back to the closest point (under L2 norm) in $\\mathcal{D}$, i.e.,\n\n```latex\n\\begin{equation}\n\\Pi_{\\mathcal{D}} [x] \\in \\argmin_{x' \\in \\mathcal{D}} \\| x - x' \\|\n\\end{equation}\n```\n\nwhich ensures that $x_{t+1} \\in \\mathcal{D}$ as desired.\n\nNow that we defined the online projected subgradient descent, let's bound the regret of such algorithm.\n\n\nAssume that $\\mathcal{D}$ is convex, closed, non-empty and bounded. In particular, there exists a constant $D$  s.t. for all $x,x' \\in \\mathcal{D}$ we have\n\n```latex\n\\begin{equation}\\label{eq:bound_D}\n    \\| x - x' \\| \\leq D\n\\end{equation}\n```\n\nalso assume that $f_t$ is $M$-lipschitz. In particular, as it is convex we have that for all $t$\n\n```latex\n\\begin{equation}\\label{eq:bound_M}\n    \\| \\nabla f_t(x_t) \\| \\leq M\n\\end{equation}\n```\n\nAnd finally set the learning rate\n\n```\n\\begin{equation}\\label{eq:eta}\n\\eta \\defeq \\frac{D}{M}\\sqrt{\\frac{1}{T}}\n\\end{equation}\n```\n\nThen we can show that the regret for the above online projected subgradient descent is \n\n```\n\\begin{equation}\n\\mathrm{Reg}_T(\\mathcal{A}) \\leq D M \\sqrt{T} = o(T)\n\\end{equation}\n```\n\ni.e. online projected sugradient descent achieves the desired sublinear regret.\n\n\n\nTo show that this is the case let's denote $\\nabla_t \\defeq\\nabla f_t(x_t) $, $x_t \\defeq x_t^{\\mathcal{A}}$ for notational convenience, and $x^* \\defeq \\argmin_{x \\in \\mathcal{X}} \\sum_t^T f_t(x)$ (which exists since $\\mathcal{D}$ is closed and convex). \nBy convexity we have \n\n```latex\n\\begin{align}\n\\mathrm{Reg}_T(\\mathcal{A}) &\\defeq \\sup_{\\{f_1, \\dots, f_T\\}} [\\sum_t^T f_t(x_t) - f_t(x^*) ] \\\\\n&\\leq \\sup_{\\{f_1, \\dots, f_T\\}} [\\sum_t^T \\nabla_t (x - x^*) ]  \\label{eq:oco_upper}\n\\end{align}\n```\n\nnow note that \n\n```latex\n\\begin{align}\n\\| x_t -x^* \\|^2 - \\| x_{t+1} - x^* \\|^2 \n&= \\| x_t -x^* \\|^2 - \\| \\Pi_{\\mathcal{D}} [x_t - \\eta \\nabla_t] - x^* \\|^2  \\\\\n&\\geq \\| x_t -x^* \\|^2 - \\| x_t - \\eta \\nabla_t - x^* \\|^2  \\\\\n&= 2 \\eta \\nabla_t \\cdot (x_t - x^*) - \\eta^2 \\| \\nabla_t \\|^2  \\\\\n\\nabla_t \\cdot (x_t - x^*) &\\leq \\frac{1}{2\\eta} (\\| x_t -x^* \\|^2 - \\| x_{t+1} - x^* \\|^2 ) + \\frac{\\eta}{2} \\| \\nabla_t \\|^2     \\label{eq:oco_ineq}\n\\end{align}\n```\n\nwhere the first inequality uses the following property of projections into convex bodies: $\\| \\Pi_D[x'] - x \\|^2 \\leq \\| x' - x \\|^2$ for any $x' \\in \\mathbb{R}^d$ and $x \\in \\mathcal{D}$.\n\nPutting back \\cref{eq:oco_ineq} into \\cref{eq:oco_upper} we get\n\n```latex\n\\begin{align}\n\\mathrm{Reg}_T &= \\sup_{\\{f_1, \\dots, f_T\\}} [\\sum_t^T f_t(x_t) - f_t(x^*) ] \\\\\n&\\leq \\sup_{\\{f_1, \\dots, f_T\\}} [\\sum_t^T (\\frac{1}{2\\eta} (\\| x_t -x^* \\|^2 - \\| x_{t+1} - x^* \\|^2 ) + \\frac{\\eta}{2} \\| \\nabla_t \\|^2) ] \\\\\n&\\leq \\sup_{\\{f_1, \\dots, f_T\\}} [ \\frac{1}{2\\eta} (\\| x_1 -x^* \\|^2 - \\| x_{T+1} - x^* \\|^2) + \\frac{\\eta}{2} \\| \\nabla_t \\|^2T ] \\label{eq:telescope}\\\\\n&\\leq \\sup_{\\{f_1, \\dots, f_T\\}} [ \\frac{1}{2\\eta} D + \\frac{\\eta}{2} \\| \\nabla_t \\|^2T ] \\label{eq:use_bound_D} & \\text{\\cref{eq:bound_D}}\\\\\n&\\leq \\sup_{\\{f_1, \\dots, f_T\\}} [ \\frac{1}{2\\eta} D^2 + \\frac{\\eta}{2}  M^2T ] \\label{eq:use_bound_M} & \\text{\\cref{eq:bound_M}}\\\\\n%\n&=  \\frac{M\\sqrt{T}}{2D} D^2 + \\frac{1}{2} \\frac{D}{M}\\sqrt{\\frac{1}{T}}  M^2T  & \\text{\\cref{eq:eta}}\\\\\n%\n&= MD\\sqrt{T}   \n\\end{align}\n```\n\nas desired where \\cref{eq:telescope} uses a telescoping sum."
  },
  {
    "category":         "Stats & ML",
    "short_name":   "information",
    "instruction":  "\nShannon's information theory has been a cornerstone in the development of modern communication systems. However, it is now used much more broadly in machine learning, statistics, and other fields. One issue with Shannon's theory is that it studies the existence of information rather than its quality or usability. Specifically, Shannonâ€™s information misses two key aspects of the downstream task. First, it is agnostic to how the actions or predictions from the decision maker (DM) will be evaluated. Second, it does not depend on the computational constraints of the DM, e.g., a polynomial-time complexity algorithm or a linear predictor. As a result, its applications in decision-making or machine learning can give rise to false claims. For example, the data processing inequality would imply that features of deeper layers in a neural network are less informative than earlier ones. Similarly, as mutual information is invariant to bijections, encrypting a message should not alter its informativeness for any DM, which is, of course, not true in practice.\n\nA potential solution to this problem is to generalize Shannon's information theory by taking a utilitarian perspective that considers both the potential actions (i.e., its computational constraints) of the decision maker and the loss function that will be used to evaluate the actions. We call this utilitarian information theory. Here's an explanation of it.\n\n## Background: Predictions, Bayes Risk, and Bayes Decision Theory with Computationally Bounded Agents\n\nIn standard Bayes decision theory, a decision maker (DM) wants to minimize some expected loss $\\ell$ when given samples from an underlying distribution $\\mathbb{P}(Y)$.\nSpecifically, the DM decides on an action $a \\in \\mathcal{A}$ and then incurs an expected loss $\\operatorname{E}_{\\mathbb{P}(Y)}[\\ell(Y,a)]$. In many scenarios, the DM can choose the optimal action after seeing outcomes from a related random variable $X$. The DM then incurs the conditional expected loss $\\operatorname{E}_{\\mathbb{P}(X)}\\left[\\inf_{a \\in \\mathcal{A}} \\operatorname{E}_{\\mathbb{P}(Y \\mid X)}[\\ell(Y,a)]\\right]$.\nReal DMs, however, are often computationally bounded, which raises the question: what if finding the optimal action $a$ is computationally intractable? What if not every action can be taken for any outcome $X$? What if the DM is constrained to taking related actions? These questions about computationally bounded DMs are important in practice but are rarely discussed in Bayes decision theory. \nTo answer these questions, it is useful to take a predictive perspective, whereby the DM has to choose a predictor $f$ from inputs $X$ to actions in $\\mathcal{A}$.\nThe conditional expected loss can then be equivalently rewritten as $\\inf_{f \\in \\mathcal{U}} \\operatorname{E}_{\\mathbb{P}(Y,X)}[\\ell(Y,f(X))]$, where the infimum is over all possible predictors $\\mathcal{U}=\\mathcal{A}^\\mathcal{X}$. The previous questions then simply amount to asking what if the DM is restricted to selecting from a subset of predictors $\\mathcal{V} \\subseteq \\mathcal{U}$ that are usually \"simple\".\nThe resulting loss incurred by the bounded DM then simply corresponds to the Bayes risk for predictive family (also called hypothesis class) $\\mathcal{V}$ and loss $\\ell$, i.e., $\\inf_{f \\in \\mathcal{V}} \\operatorname{E}_{\\mathbb{P}(Y,X)}[\\ell(Y,f(X))]$, which is well studied in statistical learning theory.\nThe predictors that achieve the Bayes risk are then called Bayes predictors $\\mathcal{V}_{p,\\ell}^*$.\nOur framework of Utilitarian Information Theory is the study of how useful some information is for this restricted DM whose actions are evaluated by general $\\ell$.\n\n## Utilitarian Information Theory\n\nThe intuition behind Shannon's conditional entropy is to measure how much uncertainty one has about some random variable $Y$ if one knows about another random variable $X$, i.e., $\\operatorname{H}(Y \\mid X) \\defeq \\inf_{f \\in \\mathcal{U}} \\operatorname{E}_{\\mathbb{P}(Y,X)}[\\ell(Y,q(X))] = \\inf_{f \\in \\mathcal{U}} \\operatorname{E}_{\\mathbb{P}(Y,X)}[\\ell(Y,q(X))]$.\nFrom the perspective of our restricted DM, it is then very natural to measure such uncertainty by the minimal loss that she would incur when predicting $Y$ given $X$, i.e., the Bayes risk. \nOur \\textit{utilitarian conditional entropy} is then naturally defined as $\\operatorname{H}_{\\ell,\\mathcal{V}}(Y \\mid X) \\defeq \\inf_{f \\in \\mathcal{V}} \\operatorname{E}_{\\mathbb{P}(Y,X)}[\\ell(Y,f(X))]$.\n\nSimilarly, Shannon's (marginal) entropy $\\operatorname{H}(Y)$ measures how much uncertainty one has about a random variable $Y$ prior to seeing any other information.\nOur \\textit{utilitarian (marginal) entropy} is then naturally defined by the uncertainty the DM has about $Y$ without additional knowledge, i.e., the unconditional Bayes risk $\\operatorname{H}_{\\ell}(Y) \\defeq \\inf_{a \\in \\mathcal{A}}\\operatorname{E}_{\\mathbb{P}(Y)}[\\ell(Y,a)]$. Note that the DM's boundedness only enters decision theory when she has access to additional knowledge $X$, so our marginal entropy does not depend on $\\mathcal{V}$.\n\nThe intuition behind Shannon's mutual information is to measure how much the knowledge of a random variable $X$ decreases your uncertainty about another $Y$, i.e., $\\operatorname{I}(Y;X) = \\operatorname{H}(Y) - \\operatorname{H}(Y \\mid X)$.\nThe same construction naturally gives rise to a notion of \\textit{utilitarian information} $\\operatorname{I}_{\\ell,\\mathcal{V}}(X;Y) = \\operatorname{H}_{\\ell}(Y) - \\operatorname{H}_{\\ell,\\mathcal{V}}(Y \\mid X)$, which measures the decrease in uncertainty for the constrained DM evaluated by $\\ell$, i.e., how much better the DM can predict $Y$ if she has access to $X$.\n\nFormally:\n\n\\begin{definition} [Action Predictive Family] Let $\\Omega_{\\text{action}} = \\lbrace f: \\mathcal{X} \\cup \\lbrace \\emptyset \\rbrace \\to \\mathcal{A} \\rbrace$, we say that $\\mathcal{V} \\subset \\Omega_{\\text{action}}$ is a predictive family (for $\\mathcal{A}$) if it satisfies \n\\begin{align} \n\\forall f \\in \\mathcal{V}, \\forall a \\in \\mathrm{range}(f), \\exists f' \\in \\mathcal{V}, \\forall x\\in\\mathcal{X}, f'(x) = a, f'(\\emptyset) = a \n\\end{align} \n\\end{definition}\n\nPredictive families essentially mean that there always exists a function that can predict any constant value for any input. This is a very mild assumption that is satisfied by most hypothesis classes in practice. We also call this property Optional Ignorance.\n\n\\begin{definition} [Utilitarian Entropy] For any loss function $\\ell$ and predictive family $\\mathcal{V}$, the utilitarian entropy of a random variable $Y$ (potentially conditioned on another random variable $X$) is defined as\n\\begin{align}\n    H_{\\ell, \\mathcal{V}}(Y) &:= \\inf_{f \\in \\mathcal{V}} \\mathbb{E}\\left[ \\ell(Y, f[\\emptyset]) \\right] \\\\\n    H_{\\ell, \\mathcal{V}}(Y|X) &:= \\inf_{f \\in \\mathcal{V}} \\mathbb{E}\\left[ \\ell(Y, f[X]) \\right]\n\\end{align} \n\\end{definition} \nIntuitively, the Utilitarian Entropy is the Bayes risk, i.e., the risk of the best prediction function.\n\n\\begin{definition}[Utilitarian Mutual Information] For any loss function $\\ell$, predictive family $\\mathcal{V}$, and random variables $X$ and $Y$, the utilitarian mutual information is defined as \n\\begin{align} \nI_{\\ell, \\mathcal{V}}(X \\to Y) := H_{\\ell, \\mathcal{V}} (Y) - H_{\\ell, \\mathcal{V}}(Y \\mid X)\n\\end{align} \n\\end{definition} \n\n\n\n## Questions\n\nFor each of the following statements, prove whether they are true or false. The proof whould be detailed.\n\n1. Utilitarian information is always symmetric, i.e., $\\operatorname{I}_{\\ell,\\mathcal{V}}(X; Y) = \\operatorname{I}_{\\ell,\\mathcal{V}}(Y; X)$.\n2. Utilitarian information is always non-negative.\n3. Utilitarian information recovers Shannon's information theory for some choices of $\\mathcal{V}$ and $\\ell$.\n4. Utilitarian information satisfies the Data Processing Inequality, i.e., the Markov Chain $Z \\to X \\to Y$ implies $\\operatorname{I}_{\\ell,\\mathcal{V}}(X; Y) \\leq \\operatorname{I}_{\\ell,\\mathcal{V}}(Z; Y)$.\n5. Utilitarian information between two random variables $X$ and $Y$ that are independent is always zero.\n6. Utilitarian information is always invariant to bijections, i.e., $\\operatorname{I}_{\\ell,\\mathcal{V}}(X; Y) = \\operatorname{I}_{\\ell,\\mathcal{V}}(b_X(X); Y)$ for any bijection $b_X$.\n\nFor each of those questions also provide an explanation / illustration of why that makes sense for machine learning. \n\n",
    "expert_solution": "\n\n## Symmetry\n\n**False.** Utilitarian information is not symmetric.\n\n*Counterexample:*  \nConsider random variables \\(X\\) taking values in \\(\\{0,1,2\\}\\) and \\(Y\\) taking values in \\(\\{0,2\\}\\). Assume the following conditional probabilities:  \n- \\(\\mathbb{P}(Y = 0 \\mid X = 0) = 1\\)  \n- \\(\\mathbb{P}(Y = 2 \\mid X = 1) = 1\\)  \n- \\(\\mathbb{P}(Y = 2 \\mid X = 2) = 1\\)  \nAnd the marginal probabilities:  \n- \\(\\mathbb{P}(X = 0) = 0.5\\)  \n- \\(\\mathbb{P}(X = 1) = 0.25\\)  \n- \\(\\mathbb{P}(X = 2) = 0.25\\)  \n\nConsider the Mean Absolute Error (MAE) as the loss function \\(\\ell\\) and the predictive family \\(\\mathcal{V}\\) consisting of the following functions:  \n- \\(c_0(\\emptyset) = 0\\), \\(c_1(\\emptyset) = 1\\), \\(c_2(\\emptyset) = 2\\)  \n- \\(f(X = 0) = 0, f(X = 1) = 2, f(X = 2) = 2\\)  \n- \\(g(\\cdot) = 1\\)  \n\nThe conditional entropy \\(\\operatorname{H}_{\\ell, \\mathcal{V}}(Y \\mid X) = 0\\) because the optimal predictor \\(f\\) can perfectly predict \\(Y\\) given \\(X\\). However, \\(\\operatorname{H}_{\\ell, \\mathcal{V}}(X \\mid Y) = 0.25\\) since \\(f\\) cannot predict \\(X = 1\\) when conditioned on \\(Y = 2\\). The marginal entropies are:  \n- \\(\\operatorname{H}_{\\ell, \\mathcal{V}}(Y) = 1\\)  \n- \\(\\operatorname{H}_{\\ell, \\mathcal{V}}(X) = 0.75\\)  \n\nTherefore, the utilitarian mutual information is:  \n\\[\n\\operatorname{I}_{\\ell, \\mathcal{V}}(X; Y) = 1 - 0 = 1, \\quad \\text{but} \\quad \\operatorname{I}_{\\ell, \\mathcal{V}}(Y; X) = 0.75 - 0.25 = 0.5.\n\\]\nSince \\(\\operatorname{I}_{\\ell, \\mathcal{V}}(X; Y) \\neq \\operatorname{I}_{\\ell, \\mathcal{V}}(Y; X)\\), utilitarian information is not symmetric.\n\n*Machine Learning Perspective:* In practical machine learning, predicting a target variable from features is often easier than the reverse. For example, in classification tasks, predicting the class label (a lower-dimensional output) from features (often high-dimensional and complex) is generally more straightforward than predicting the features given the class label.\n\n## Non-Negativity\n\n**True.** Utilitarian information is always non-negative.\n\n*Proof:*  \nBy definition, a predictive family \\(\\mathcal{V}\\) is constructed to include constant predictors such that for every action \\(a \\in \\mathcal{A}\\), there exists a constant predictor \\(f_a \\in \\mathcal{V}\\) satisfying:\n\\[\nf_a(X) = a, \\quad \\forall X.\n\\]\nThis implies:\n\\[\n\\operatorname{H}_{\\ell, \\mathcal{V}}(Y \\mid X) = \\inf_{f \\in \\mathcal{V}} \\operatorname{E}_{\\mathbb{P}(Y, X)}[\\ell(Y, f(X))] \\leq \\inf_{a \\in \\mathcal{A}} \\operatorname{E}_{\\mathbb{P}(Y)}[\\ell(Y, a)] = \\operatorname{H}_{\\ell}(Y).\n\\]\nThus:\n\\[\n\\operatorname{I}_{\\ell,\\mathcal{V}}(X; Y) = \\operatorname{H}_{\\ell}(Y) - \\operatorname{H}_{\\ell,\\mathcal{V}}(Y \\mid X) \\geq 0.\n\\]\n\n*Machine Learning Perspective:* In machine learning, having access to additional features or information can never worsen a model's performance since it always has the option not to use that information. Thus, additional information can only help or maintain the current state, ensuring non-negativity.\n\n## Recovery of Shannon's Information Theory\n\n**True.** Utilitarian information can recover Shannon's information theory for specific choices of \\(\\mathcal{V}\\) and \\(\\ell\\).\n\n*Proof:*  \nConsider the logarithmic loss (cross-entropy loss), which is common in information theory and machine learning. For a predicted probability distribution \\(q(Y)\\) and the true distribution \\(p(Y)\\), the logarithmic loss is:\n\\[\n\\ell(Y, q) = -\\log q(Y).\n\\]\nThe expected logarithmic loss over a distribution \\(\\mathbb{P}(Y)\\) is:\n\\[\n\\operatorname{E}_{\\mathbb{P}(Y)}[\\ell(Y, q)] = -\\operatorname{E}_{\\mathbb{P}(Y)}[\\log q(Y)].\n\\]\nThe Bayes risk for this loss function is achieved when \\(q = p\\), the true conditional distribution of \\(Y\\) given \\(X\\). This Bayes risk corresponds to the conditional entropy:\n\\[\n\\operatorname{H}(Y \\mid X) = -\\operatorname{E}_{\\mathbb{P}(Y, X)}[\\log \\mathbb{P}(Y \\mid X)].\n\\]\n\n*Choice of Predictive Family \\(\\mathcal{V}\\):*  \nLet \\(\\mathcal{V}\\) be the set of all possible conditional distributions \\(q(Y \\mid X)\\), i.e., the set of all functions \\(f: \\mathcal{X} \\to \\Delta_{\\mathcal{Y}}\\) where \\(\\Delta_{\\mathcal{Y}}\\) is the set of all probability distributions over \\(\\mathcal{Y}\\). This choice of \\(\\mathcal{V}\\) includes all probabilistic mappings from \\(X\\) to \\(Y\\), and is effectively unrestricted.\n\nUtilitarian mutual information with logarithmic loss and predictive family \\(\\mathcal{V}\\) is:\n\\[\n\\operatorname{I}_{\\ell, \\mathcal{V}}(X; Y) = \\operatorname{H}_{\\ell}(Y) - \\operatorname{H}_{\\ell, \\mathcal{V}}(Y \\mid X).\n\\]\nGiven the logarithmic loss as a proper scoring rule, the infimum of expected loss is attained when the predicted distribution matches the true distribution. Thus:\n\\[\n\\operatorname{I}_{\\ell, \\mathcal{V}}(X; Y) = \\operatorname{H}(Y) - \\operatorname{H}(Y \\mid X) = \\operatorname{I}(X; Y).\n\\]\nThus, by selecting appropriate \\(\\ell\\) and \\(\\mathcal{V}\\), utilitarian information theory recovers Shannon's information theory.\n\n*Machine Learning Perspective:* When unconstrained (having full access to all features and complexity), utilitarian information theory aligns with Shannon's framework, retaining all its properties.\n\n## Data Processing Inequality\n\n**False.** Utilitarian information does not necessarily satisfy the Data Processing Inequality.\n\n*Counterexample:*  \nConsider random variables \\(X\\) taking values in \\(\\{0,2\\}\\) and \\(Y\\) taking values in \\(\\{0,1,2\\}\\). Assume:  \n- \\(\\mathbb{P}(Y = 0 \\mid X = 0) = 1\\)  \n- \\(\\mathbb{P}(Y = 2 \\mid X = 2) = \\mathbb{P}(Y = 1 \\mid X = 2) = 0.5\\)  \n- \\(\\mathbb{P}(X = 0) = \\mathbb{P}(X = 2) = 0.5\\)  \n\nUsing MAE loss \\(\\ell\\) and the predictive family \\(\\mathcal{V}\\) consisting of functions:  \n- \\(c_0(\\emptyset) = 0, c_1(\\emptyset) = 1, c_2(\\emptyset) = 2\\)  \n- \\(f(X = 0) = 0, f(X = 1) = 1, f(X = 2) = 11\\)  \n- \\(g(\\cdot) = 2\\)  \n\nThe utilitarian marginal and conditional entropies are both 1. Predicting with \\(f\\) yields a higher risk since it cannot predict well when \\(X = 2\\). The risk using \\(f\\) is \\(0 + 10 \\times 0.25 + 11 \\times 0.25 = 5.25\\). Predicting with \\(c_0\\) yields a risk of 0.75. Thus, \\(\\operatorname{I}_{\\ell, \\mathcal{V}}(X; Y) = 0.75\n\n - 0.75 = 0\\). However, if a function \\(h\\) exists such that \\(h(2) = 1\\) and is the identity otherwise, then \\(f\\) applied to \\(h(X)\\) becomes a good predictor, making \\(\\operatorname{I}_{\\ell, \\mathcal{V}}(h(X); Y) = 0.75 - 0.25 = 0.5\\). Given \\(X - h(X) - Y\\) is a Markov chain, \\(\\operatorname{I}_{\\ell, \\mathcal{V}}(h(X); Y) = 0.5 > 0 = \\operatorname{I}_{\\ell, \\mathcal{V}}(X; Y)\\), violating the Data Processing Inequality.\n\n*Machine Learning Perspective:* Feature preprocessing can enhance the \"extractability\" of information by a predictor, even if a preprocessing function is bijective. This reflects practical scenarios where feature engineering or transformations can increase predictive power.\n\n## Independence\n\n**True.** Utilitarian information between two independent random variables \\(X\\) and \\(Y\\) is always zero.\n\n*Proof:*  \nUsing independence:\n\\[\n\\operatorname{H}_{\\ell, \\mathcal{V}}(Y \\mid X) = \\inf_{f \\in \\mathcal{V}} \\operatorname{E}_{(X, Y) \\sim \\mathbb{P}(X, Y)}[\\ell(Y, f(X))] = \\inf_{f \\in \\mathcal{V}} \\operatorname{E}_{X \\sim \\mathbb{P}(X)} \\operatorname{E}_{Y \\sim \\mathbb{P}(Y)}[\\ell(Y, f(X))].\n\\]\nBy Jensen's inequality and the assumption of Optional Ignorance, we get:\n\\[\n\\operatorname{H}_{\\ell, \\mathcal{V}}(Y \\mid X) \\geq \\operatorname{E}_{X \\sim \\mathbb{P}(X)} \\left[ \\inf_{f \\in \\mathcal{V}} \\operatorname{E}_{Y \\sim \\mathbb{P}(Y)}[\\ell(Y, f(X))] \\right] = \\operatorname{H}_{\\ell, \\mathcal{V}}(Y).\n\\]\nThus:\n\\[\n\\operatorname{I}_{\\ell, \\mathcal{V}}(X; Y) = \\operatorname{H}_{\\ell, \\mathcal{V}}(Y) - \\operatorname{H}_{\\ell, \\mathcal{V}}(Y \\mid X) \\leq 0.\n\\]\nCombining with non-negativity, we have \\(\\operatorname{I}_{\\ell, \\mathcal{V}}(X; Y) = 0\\).\n\n*Machine Learning Perspective:* When two variables are independent, they provide no information about each other. This is fundamental in machine learning, where independent features or labels do not inform predictions.\n\n## Invariance to Bijections\n\n**False.** Utilitarian information is not always invariant to bijections.\n\n*Counterexample:*  \nConsider the previous example where a function \\(h\\) was a bijection that improved mutual information. Applying a function can increase utilitarian information by enabling the constrained predictor to better predict the target variable. This makes sense for machine learning because preprocessing features can often enhance predictive accuracy, even if the transformation is bijective.\n\n**Conclusion:** Utilitarian information theory provides a more nuanced approach to information theory in the context of machine learning by incorporating computational constraints and task-specific loss functions. This aligns well with practical scenarios where perfect information is unattainable, and decision-makers must operate within computational and informational limits."
  }
]